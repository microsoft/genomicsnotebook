{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "# Convert Synthetic FHIR and PacBio VCF Data to parquet and Explore with Azure Synapse Analytics\n",
    "### Overview\n",
    "In **Sections 1-4**, this notebook generates synthetic FHIR data using Synthea, uploads the data to a new FHIR server, and downloads the data in Parquet format.  \n",
    "In **Section 5**, this notebook assumes that PacBio VCFs are available as a second input, and converts them to Parquet using `bcftools` and `pandas`.  \n",
    "In **Section 6**, Azure Synapse Analytics is then used to perform joint queries on both datasets.\n",
    "\n",
    "#### Please click this [link](https://github.com/microsoft/genomicsnotebook/blob/main/docs/fhir_long_read_1.JPG) to view the high level implementation design of this notebook.\n",
    "### Contents\n",
    "[1. Generate Synthetic FHIR Data with Synthea](#1)  \n",
    "[2. Configure a FHIR Server](#2)  \n",
    "[3. Import Synthea Data to the FHIR Server](#3)  \n",
    "[4. Set up a FHIR->Synapse Sync Agent](#4)  \n",
    "[5. Convert PacBio VCF data to Parquet](#5)  \n",
    "[6. Use Azure Synapse for Analytics](#6)  \n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic FHIR Data with Synthea<a id=\"2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "According to their project's Github,\n",
    "> Synthea[<sup>[1]</sup>](#r1) is a Synthetic Patient Population Simulator.  \n",
    "> The goal is to output synthetic, realistic (but not real), patient data and associated health records in a variety of formats.  \n",
    "\n",
    "The health record output format used in this notebook is \"Fast Healthcare Interoperability Resources\", or FHIR[<sup>[2]</sup>](#r2).  \n",
    "FHIR is the leading standard for health care data exchange, published by HL7.  \n",
    "In order to set up Synthea[<sup>[1]</sup>](#r1), first reinstall JDK (otherwise `javadoc` fails during Gradle build)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install -y default-jdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the latest stable version of the Synthea[<sup>[1]</sup>](#r1) git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/synthetichealth/synthea.git -b v3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to `synthea/` prior to building/running Synthea[<sup>[1]</sup>](#r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd synthea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Synthea[<sup>[1]</sup>](#r1) using Gradle 7.0.2 and Java 11.0.15. On Azure `DS11_V2`, this takes anywhere from 30 minutes to 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./gradlew build test check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Verify that the build/installation has succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_synthea -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Generate a test dataset of 10 patients (Synthea[<sup>[1]</sup>](#r1) only counts `Alive` patients, so there will likely be more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run([\"./run_synthea\",\n",
    "    \"-s\", \"42\",\n",
    "    \"-cs\", \"99\",\n",
    "    \"-p\", \"10\",\n",
    "    f'--exporter.baseDirectory=/mnt/batch/tasks/shared/LS_root/mounts/clusters/<USERNAME>/code'\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the Synthea[<sup>[1]</sup>](#r1) data generation succeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /mnt/batch/tasks/shared/LS_root/mounts/clusters/<USERNAME>/code | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure a FHIR Server <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.0) Identify Resource Group and Subscription**\n",
    "- Navigate to your desired Azure \"Resource Group\", note the name -> \"Overview\" -> copy \"Subscription ID\"\n",
    "- Set `resource_group` and `sub_id` in [Section 3.1](#globals)\n",
    "\n",
    "**2.1) Create an \"Azure API for FHIR\"[<sup>[3]</sup>](#r3) instance**, named `<fhir_server>`\n",
    "- Navigate to `https://<fhir_server>.azurehealthcareapis.com/metadata` and verify a \"Capability Statement\" is retrieved.  \n",
    "That means the FHIR server[<sup>[3]</sup>](#r3) is running.\n",
    "- Set `fhir_server` in [Section 3.1](#globals)\n",
    "- Use RBAC[<sup>[6]</sup>](#r6): `<fhir_server>` left pane \"Identity\" -> \"On\" -> \"Save\"\n",
    "\n",
    "**2.2) Register an App** with permission to read/write data to the FHIR server[<sup>[3]</sup>](#r3) (this notebook will be that \"app\" and use those permissions)\n",
    "- Create App: \"Azure Active Directory\"[<sup>[4]</sup>](#r4) -> left pane \"App Registrations -> top bar \"New Registration\" -> name `<fhir_app>` -> \"Register\"\n",
    "- Navigate to App: \"Azure Active Directory\"[<sup>[4]</sup>](#r4) -> left pane \"App Registrations\" -> select `<fhir_app>` -> left pane \"Overview\"\n",
    "- Copy the \"Application (client) ID\" and \"Directory (tenant) ID\", then set `client_id` and `tenant_id` in [Section 3.1](#globals)\n",
    "- *More information on app registration:* [[5]](#r5)\n",
    "\n",
    "**2.3) Create Client Secret** for this notebook to prove that it is the \"app\", or client <a id=\"secret\"></a>\n",
    "- Navigate to App: \"Azure Active Directory\"[<sup>[4]</sup>](#r4) -> left pane \"App Registrations\" -> select `<fhir_app>`\n",
    "- Create Secret: left pane \"Certificates & Secrets\" -> \"+ New Client Secret\" -> name `<jnb_secret>` -> Add\n",
    "- Save Secret: copy `<jnb_secret>`'s `Value`, and store as `client_secret` in [Section 3.1](#globals). If you do not copy the `Value` immediately after creation, you will no longer be able to access it, and will need to create a new secret.\n",
    "- *More information on app registration:* [[5]](#r5)\n",
    "\n",
    "**2.4) Add Permissions** for this notebook to POST/GET data from the FHIR server[<sup>[3]</sup>](#r3)\n",
    "- Navigate to \"Azure API for FHIR\" server[<sup>[3]</sup>](#r3) named `<fhir_server>`\n",
    "- Select Role: left pane \"Access Control (IAM)\" -> top bar \"+ Add\" -> \"Add role assignment\" -> Role=\"FHIR Data Contributor\"\n",
    "- Select Members: middle tab \"Members\" -> \"Assign access to: User...\" -> \"+ Select members\" -> search `<fhir_app>` (created in step 1) -> \"Select\" -> Review & Assign\n",
    "- *More information on Azure Role-Based Access Control:* [[6]](#r6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Synthea Data to the FHIR Server <a id=\"4\"></a>\n",
    "We wrote the script below based on an auto-generated Postman[<sup>[11]</sup>](#r11) template.  \n",
    "Postman[<sup>[11]</sup>](#r11) is a platform for using REST APIs, and there's a tutorial for using it with FHIR here: [[12]](#r12).\n",
    "\n",
    "**3.1) Set globals** for querying the FHIR API <a id=\"globals\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group = \"<resource_group>\"\n",
    "sub_id = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
    "\n",
    "fhir_server = \"<fhir_server>\"\n",
    "fhir_url = f\"https://{fhir_server}.azurehealthcareapis.com\"\n",
    "\n",
    "tenant_id = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
    "client_id = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
    "client_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from glob import glob\n",
    "from urllib.parse import urlencode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2) Request an access token**, using the [previously-generated client secret](#secret).\n",
    "- *More information on Azure AD Access Tokens:* [[7]](#r7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set request parameters\n",
    "token_url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "payload = {\n",
    "    'grant_type': 'Client_Credentials',\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "    'resource': fhir_url\n",
    "}\n",
    "headers = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded'\n",
    "}\n",
    "\n",
    "# request token from server\n",
    "response = requests.request(\"POST\", token_url, headers=headers, data=urlencode(payload))\n",
    "content = json.loads(response.content)\n",
    "\n",
    "# save token from response\n",
    "if response.status_code == 200:\n",
    "    print(f\"{content['token_type']} access token retrieved for {content['resource']} successfully.\")\n",
    "    bearer_token = content[\"access_token\"]\n",
    "else:\n",
    "    print(f\"ERROR: unexpected status code {response.status_code}.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3) Add patient data** generated by Synthea[<sup>[1]</sup>](#r1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_url = f\"{fhir_url}/Patient\"\n",
    "\n",
    "# parse all Synthea-generated JSON data\n",
    "for filename in glob(f\"/home/azureuser/cloudfiles/data/datastore/synthea/fhir/*.json\"):\n",
    "    json_file = open(filename, 'r')\n",
    "    json_obj = json.load(json_file)['entry']\n",
    "    for resource in json_obj:\n",
    "        \n",
    "        # skip all non-patient resources (encounters, providers, etc...)\n",
    "        if resource[\"resource\"][\"resourceType\"] == \"Patient\":\n",
    "            \n",
    "            # add patient to FHIR database using REST API\n",
    "            payload = json.dumps(resource[\"resource\"])\n",
    "            headers = {\n",
    "              'Authorization': f'Bearer {bearer_token}',\n",
    "              'Content-Type': 'application/json'\n",
    "            }\n",
    "\n",
    "            # verify success\n",
    "            response = requests.request(\"POST\", patient_url, headers=headers, data=payload)\n",
    "            if response.status_code >= 200 and response.status_code < 300:\n",
    "                print(f\"Patient {' '.join(json.loads(response.content)['name'][0]['given'])} {json.loads(response.content)['name'][0]['family']} added successfully.\")\n",
    "            else:\n",
    "                print(f\"ERROR: unexpected status code {response.status_code}.\")\n",
    "                print(response.text)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the first 10 `Patient`s from the FHIR database to verify the previous step succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"{fhir_url}/Patient\"\n",
    "headers = {'Authorization': f'Bearer {bearer_token}'}\n",
    "response = requests.request(\"GET\", url, headers=headers, data={})\n",
    "try:\n",
    "    print(f\"{len(json.loads(response.content)['entry'])} patients total\")\n",
    "except KeyError:\n",
    "    print(\"No patients in FHIR database.\")\n",
    "#json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all `Patient`s (use ONLY for resetting FHIR database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # looping is required, since GET will only fetch first 10 patients\n",
    "# while True:\n",
    "#     # query for list of all patients\n",
    "#     url = f\"{fhir_url}/Patient\"\n",
    "#     headers = {'Authorization': f'Bearer {bearer_token}'}\n",
    "#     response = requests.request(\"GET\", url, headers=headers, data={})\n",
    "#     fhir_data = json.loads(response.content)\n",
    "\n",
    "#     # delete each patient (in chunk of 10)\n",
    "#     try:\n",
    "#         for patient in fhir_data[\"entry\"]:\n",
    "#             url = f\"{fhir_url}/Patient/{patient['resource']['id']}\"\n",
    "#             response = requests.request(\"DELETE\", url, headers=headers, data={})\n",
    "#             if response.status_code >= 200 and response.status_code < 300:\n",
    "#                 print(f\"Patient {' '.join(patient['resource']['name'][0]['given'])} {patient['resource']['name'][0]['family']} deleted successfully.\")\n",
    "#             else:\n",
    "#                 print(f\"ERROR: unexpected status code {response.status_code}.\")\n",
    "#                 print(response.text)\n",
    "#                 break\n",
    "#     except KeyError:\n",
    "#         print(\"Done!\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up the FHIR->Synapse Sync Agent <a id=\"5\"></a>\n",
    "This notebook section follows the \"FHIR to Synapse Sync Agent\" tutorial provided Microsoft's \"FHIR Analytics Pipelines\" Github repository[<sup>[13]</sup>](#r13).\n",
    "\n",
    "**4.1) Deploy the custom Azure template** provided by the \"FHIR to Synapse Sync Agent\" tutorial[<sup>[13]</sup>](#r13).\n",
    "- Navigate to the Github repo by clicking [this link](https://github.com/microsoft/FHIR-Analytics-Pipelines/blob/main/FhirToDataLake/docs/Deploy-FhirToDatalake.md).\n",
    "- Scroll down to \"Deployment\", then \"1. Deploy the Pipeline\", and click the blue button \"Deploy to Azure\"\n",
    "- App Name `<sync_agent>` -> set FHIR URL -> Authentication `true` -> Container Name `fhir` -> \"Review + Create\" -> \"Create\"\n",
    "- Note: DO NOT have any dashes or underscores in the container name, or else it will fail silently\n",
    "\n",
    "**4.2) Add Permissions** for Function App `<sync_agent>` to read FHIR data\n",
    "- `<sync agent>` left pane \"Identity\" ->  \"Azure role assignments\" -> \"Resource Group\" -> `<resource_group>` -> \"FHIR Data Contributor\"\n",
    "\n",
    "**4.3) Mount Parquet Data** on the running AzureML[<sup>[0]</sup>](#r0) machine.\n",
    "- Add Datastore: `<azure_ml>` left pane \"Datastores\" -> \"+ New datastore\" -> name `fhir` -> \"Azure Blob Storage\" -> `<sync_agent_storage>` -> `fhir` -> `<keys from next step>` -> \"Create\"\n",
    "- Get Storage keys: `<sync_agent_storage>` left pane \"Access keys\" -> \"Show\" -> \"Copy to clipboard\"\n",
    "- Mount Datastore: `<azure_ml>` left pane \"Compute\" -> `<azure_ml>` instance -> top bar \"Data (preview)\" -> \"Mount\" -> \"Azure Storage\" -> select `fhir` -> name `fhir`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert PacBio VCF data to Parquet <a id=\"6\"></a>\n",
    "\n",
    "**5.1) Mount the PacBio container** on the running AzureML[<sup>[0]</sup>](#r0) machine.\n",
    "\n",
    "- Add Datastore: `<azure_ml>` left pane \"Datastores\" -> \"+ New datastore\" -> name `pacbio` -> \"Azure Blob Storage\" -> `<storage_account>` -> `pacbio` -> `<keys from next step>` -> \"Create\"\n",
    "- Get Storage keys: `<storage_account>` left pane \"Access keys\" -> \"Show\" -> \"Copy to clipboard\"\n",
    "- Mount Datastore: `<azure_ml>` left pane \"Compute\" -> `<azure_ml>` instance -> top bar \"Data (preview)\" -> \"Mount\" -> \"Azure Storage\" -> select `pacbio` -> name `pacbio`\n",
    "\n",
    "**5.2) Install `bcftools`**  \n",
    "The `bcftools` documentation[<sup>[14]</sup>](#r14) can be found [here](https://samtools.github.io/bcftools/bcftools.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y bcftools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3) Convert all PacBio VCFs to TSV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create `pacbio_tsv` and `pacbio_parquet` containers in the same storage account as the `pacbio` VCF data. \n",
    "- Mount them to the AzureML instance (as in Section 5.1).\n",
    "- Convert all VCF data to TSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os, subprocess\n",
    "import pandas as pd\n",
    "pb_dir = \"/***/pacbio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vcf_fn in glob(f\"{pb_dir}/**/*.vcf.gz\", recursive=True): # no gvcfs\n",
    "#for vcf_fn in glob(f\"{pb_dir}/**/*.*vcf.gz\", recursive=True): # allow gvcfs\n",
    "    patient = vcf_fn.split(\"/\")[7]\n",
    "    print(f\"Converting Patient {patient} data to TSV.\")\n",
    "    subprocess.run([\"bcftools\", \"query\", \n",
    "        \"--print-header\",\n",
    "        \"-f\", \"%CHROM\\t%POS\\t%TYPE\\t%REF\\t%ALT[\\t%GT]\\n\", vcf_fn,\n",
    "        \"-o\", f\"{pb_dir}_tsv/{patient}.tsv\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4) Convert TSVs to Parquet**  \n",
    "TSV data can be converted to Parquet[<sup>[15]</sup>](#r15) format using `pandas`[<sup>[16]</sup>](#r16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pb_dfs = []\n",
    "for tsv_fn in glob(f\"{pb_dir}_tsv/*.tsv\"):\n",
    "    \n",
    "    # read input .tsv\n",
    "    patient = os.path.splitext(tsv_fn.split(\"/\")[-1])[0]\n",
    "    print(f\"converting {patient}...\", end=\"\")\n",
    "    pb_df = pd.read_csv(tsv_fn, delimiter=\"\\t\")\n",
    "    \n",
    "    # new column names\n",
    "    cols = []\n",
    "    for col in pb_df.columns:\n",
    "        new_col = col.split(\"]\")[1]\n",
    "        if \":\" in new_col:\n",
    "            new_col = new_col[list(new_col).index(\":\")+1:]\n",
    "        cols.append(new_col)\n",
    "    pb_df.columns = cols\n",
    "    pb_df['PATIENT'] = patient\n",
    "    \n",
    "    # add df to list\n",
    "    all_pb_dfs.append(pb_df)\n",
    "    print(\"done!\")\n",
    "    \n",
    "# output to .parquet\n",
    "pb_df = pd.concat(all_pb_dfs)\n",
    "pb_df.to_parquet(f\"{pb_dir}_parquet/10_patients.parquet\")\n",
    "print(pb_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_df = pd.read_parquet(f\"{pb_dir}_parquet/10_patients.parquet\")\n",
    "pb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Use Azure Synapse for Analytics to Explore Parquet files <a id=\"7\"></a>\n",
    "Azure Synapse Analytics[<sup>[17]</sup>](#r17) is an enterprise-scale data analytics service, perfect for working with large datasets.  \n",
    "\n",
    "Please transfer over to Azure Synapse to load our Parquet data into a Synapse workspace with the following sample commands:\n",
    "\n",
    "For further sample tables and queries, please visit https://techcommunity.microsoft.com/t5/healthcare-and-life-sciences/combine-and-explore-fhir-server-and-genomics-data-in-azure/ba-p/3298335"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample command: Import synthetic FHIR data into Azure Synapse Studio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "df_fhir = spark.read.load('abfss://<PARQUET LOCATION>@<SYNAPSE STORAGE>.dfs.core.windows.net/FILENAME.parquet', format='parquet')\n",
    "df_fhir.createOrReplaceTempView(\"fhir_table\")\n",
    "display(df_fhir.limit(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Command: Import sample PacBio data to into Azure Synapse Studio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "blob_account_name = \"<STORAGE ACCOUNT NAME>\"\n",
    "blob_container_name = \"pacbio-parquet\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession.builder.getOrCreate()\n",
    "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
    "blob_sas_token = token_library.getConnectionString(\"<GENOMICS PARQUET FILE LOCATION>\")\n",
    "\n",
    "spark.conf.set(\n",
    "    'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "    blob_sas_token)\n",
    "df_pacbio = spark.read.load('wasbs://pacbio-parquet@<STORAGE ACCOUNT NAME>.blob.core.windows.net/FILENAME.parquet', format='parquet')\n",
    "df_pacbio.createOrReplaceTempView(\"pacbio_table\")\n",
    "display(df_pacbio.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a id=\"references\"></a>\n",
    "[0]  <a id=\"r0\"></a> Azure Machine Learning: https://docs.microsoft.com/en-us/azure/machine-learning/  \n",
    "[1]  <a id=\"r1\"></a> Walonoski, Jason, et al. \"Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record.\" Journal of the American Medical Informatics Association 25.3 (2018): 230-238. The MITRE Corporation. https://github.com/synthetichealth/synthea  \n",
    "[2]  <a id=\"r2\"></a> FHIR HL7: http://hl7.org/fhir/index.html  \n",
    "[3]  <a id=\"r3\"></a> Azure API for FHIR: https://docs.microsoft.com/en-us/azure/healthcare-apis/fhir/overview  \n",
    "[4] <a id=\"r4\"></a> Azure Active Directory: https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-whatis  \n",
    "[5] <a id=\"r5\"></a> Azure App Registration: https://docs.microsoft.com/en-us/azure/healthcare-apis/register-application  \n",
    "[6] <a id=\"r6\"></a> Azure Role Based Access Control (RBAC): https://docs.microsoft.com/en-us/azure/healthcare-apis/azure-api-for-fhir/configure-azure-rbac  \n",
    "[7] <a id=\"r7\"></a> Azure AD Access Tokens: https://docs.microsoft.com/en-us/azure/healthcare-apis/azure-api-for-fhir/azure-active-directory-identity-configuration  \n",
    "[8] <a id=\"r8\"></a> Microsoft `fhir-loader`: https://github.com/microsoft/fhir-loader  \n",
    "[9] <a id=\"r9\"></a> Azure `bulk-import`: https://docs.microsoft.com/en-us/azure/healthcare-apis/fhir/configure-import-data  \n",
    "[10] <a id=\"r10\"></a> Azure Healthcare APIs changelog: https://docs.microsoft.com/en-us/azure/templates/microsoft.healthcareapis/change-log/services  \n",
    "[11] <a id=\"r11\"></a> Postman API Platform: https://www.postman.com/  \n",
    "[12] <a id=\"r12\"></a> Postman FHIR Tutorial: https://docs.microsoft.com/en-us/azure/healthcare-apis/fhir/use-postman  \n",
    "[13] <a id=\"r13\"></a> FHIR to Synapse Sync Agent Tutorial: https://github.com/microsoft/FHIR-Analytics-Pipelines/blob/main/FhirToDataLake/docs/Deployment.md  \n",
    "[14] <a id=\"r14\"></a> `bcftools` Documentation: https://samtools.github.io/bcftools/bcftools.html  \n",
    "[15] <a id=\"r15\"></a> Parquet File Format: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html  \n",
    "[16] <a id=\"r16\"></a> Python `pandas` library: https://pandas.pydata.org/  \n",
    "[17] <a id=\"r17\"></a> Azure Synapse Analytics: https://docs.microsoft.com/en-us/azure/synapse-analytics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "THIS NOTEBOOK JUST PROVIDE A SAMPLE CODES FOR EDUCATIONAL PURPOSES. MICROSOFT DOES NOT CLAIM ANY OWNERSHIP ON THESE CODES AND LIBRARIES. MICROSOFT PROVIDES THIS NOTEBOOK AND SAMPLE USE OF  LIBRARIES ON AN “AS IS” BASIS. DATA OR ANY MATERIAL ON THIS NOTEBOOK. MICROSOFT MAKES NO WARRANTIES, EXPRESS OR IMPLIED, GUARANTEES OR CONDITIONS WITH RESPECT TO YOUR USE OF THIS NOTEBOOK. TO THE EXTENT PERMITTED UNDER YOUR LOCAL LAW, MICROSOFT DISCLAIMS ALL LIABILITY FOR ANY DAMAGES OR LOSSES, INCLUDING DIRECT, CONSEQUENTIAL, SPECIAL, INDIRECT, INCIDENTAL OR PUNITIVE, RESULTING FROM YOUR USE OF THIS NOTEBOOK.\n",
    "\n",
    "#### Notebook prepared by [Tim Dunn](https://github.com/TimD1)- Research Intern- Microsoft Biomedical Platforms and Genomics"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
