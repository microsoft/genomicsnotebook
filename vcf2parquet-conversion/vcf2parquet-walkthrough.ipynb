{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import explode, col, lit, xxhash64\n","\n","# Import glow.py and register Glow package\n","import glow\n","glow.register(spark)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42f3599e-f63c-479f-92d7-846e09cd39ac"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Configure session credentials\n","# Set up a SAS for a container with public data - no changes needed here (public SAS)\n","\n","spark.conf.set(\n","  \"fs.azure.sas.dataset.dataset1000genomes.blob.core.windows.net\",\n","  \"sv=2019-10-10&si=prod&sr=c&sig=9nzcxaQn0NprMPlSh4RhFQHcXedLQIcFgbERiooHEqM%3D\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32cd967d-57af-4c71-8249-6bb2a44c65f2"}},"outputs":[],"execution_count":null},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Provide your storage account, container and SAS token\n","outputStorageAccount =  \n","outputContainer = \n","outputSAS = \n","outputDir = "]},{"cell_type":"code","source":["# Set up a SAS for a container to store .parquet files\n","spark.conf.set(\n","  \"fs.azure.sas.\"+outputContainer+\".\"+outputStorageAccount+\".blob.core.windows.net\", outputSAS)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3206b3ae-b423-4ea7-9571-da6b29220976"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# List files for latest release (20130502)\n","\n","dbutils.fs.ls(\"wasbs://dataset@dataset1000genomes.blob.core.windows.net/release/20130502\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ccaf817-b3a6-4021-aaa2-3f7dbc75736b"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Read in data for chr 22 with flatten info fields and sample ids\n","\n","source = \"wasbs://dataset@dataset1000genomes.blob.core.windows.net/release/20130502/ALL.chr22*.vcf.gz\"\n","\n","data = spark.read\\\n","  .format(\"vcf\")\\\n","  .option(\"includeSampleIds\", True)\\\n","  .option(\"flattenInfoFields\", True)\\\n","  .load(source)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"386c7ba3-4fc5-457b-bc88-94513f811902"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Look at regular sites\n","\n","data.where(\"INFO_MULTI_ALLELIC = FALSE\").show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddd81d9f-b7ef-4c1d-9659-ce5d0658981a"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Look at multiallelic sites\n","\n","data.where(\"INFO_MULTI_ALLELIC = TRUE\").show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3c28ccc-5469-4c12-86d5-0ca719a49614"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Let's add hashId column, hash is built on all columns except genotypes\n","# Later hasId might be used as unique id for variants \n","\n","hashCols = list(set(data.columns) - {'genotypes'})\n","dataHashed = data.withColumn('hashId', xxhash64(*hashCols))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"267b97bc-c0f8-4266-8db8-dc00c3390e3c"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Write out data in .parquet format to your storage account\n","\n","hashVariants = True \n","\n","if hashVariants:\n","  sink = \"wasbs://\"+outputContainer + \"@\" + outputStorageAccount + \".blob.core.windows.net\"+ outputDir + \"/hashed/chr22\"\n","  dataHashed.write. \\\n","    mode(\"overwrite\"). \\\n","    format(\"parquet\"). \\\n","    save(sink)\n","else:\n","  sink = \"wasbs://\"+outputContainer + \"@\" + outputStorageAccount + \".blob.core.windows.net\"+ outputDir + \"/original/chr22\" \n","  data.write. \\\n","    mode(\"overwrite\"). \\\n","    format(\"parquet\"). \\\n","    save(sink)\n","  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cb2db54-fcab-43c1-94f7-de6d64f20742"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Explode on genotypes\n","\n","dataExploded = dataHashed.withColumn('genotypes', explode('genotypes'))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80bb59ab-56aa-4c37-b30c-0438e207af9e"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Flatten struct columns - genotypes column in this case\n","\n","def flattenStructFields(df):\n","  flat_cols = [c[0] for c in df.dtypes if c[1][:6] != 'struct']\n","  nested_cols = [c[0] for c in df.dtypes if c[1][:6] =='struct']\n","  flat_df = df.select(flat_cols + \n","                     [col(nc+'.'+c).alias(nc+'_'+c)\n","                     for nc in nested_cols\n","                     for c in df.select(nc+'.*').columns])\n","  return flat_df\n","\n","dataExplodedFlatten = flattenStructFields(dataExploded)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b40bfe7-542e-4549-bc70-a4abf4da9ce0"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Look at regular sites\n","display(dataExplodedFlatten.where(\"INFO_MULTI_ALLELIC = FALSE\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bfcec81-3da4-4c8a-b893-e570a2411ff5"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Look at multiallelic sites\n","display(dataExplodedFlatten.where(\"INFO_MULTI_ALLELIC = TRUE\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a23dc1e-bf20-4680-87db-493367e67f07"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# That's final point where all transformations will be made, it'll be long and expensive step - you might need bigger cluster to complete it\n","# Write out exploded data to your storage account\n","\n","sink = \"wasbs://\"+outputContainer + \"@\" + outputStorageAccount + \".blob.core.windows.net\"+ outputDir + \"/exploded/chr22\" \n","\n","dataExplodedFlatten.write. \\\n","  mode(\"overwrite\"). \\\n","  format(\"parquet\"). \\\n","  save(sink)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"091f7803-4fd9-4826-a8de-a9e6e423812a"}},"outputs":[],"execution_count":null}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"vcf2parquet","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3635319616110480}},"nbformat":4,"nbformat_minor":0}