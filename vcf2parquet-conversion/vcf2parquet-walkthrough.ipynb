{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "1. Import packages, register Glow, configure credentials to access 1000 genomes dataset from Azure Genomics Data Lake and read data for chromosome 22."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "32cd967d-57af-4c71-8249-6bb2a44c65f2"
        }
      },
      "source": [
        "from pyspark.sql.functions import explode, col, lit, xxhash64\n",
        "\n",
        "import glow\n",
        "spark = glow.register(spark)\n",
        "\n",
        "spark.conf.set(\n",
        "  'fs.azure.sas.dataset.dataset1000genomes.blob.core.windows.net',\n",
        "  'sv=2019-10-10&si=prod&sr=c&sig=9nzcxaQn0NprMPlSh4RhFQHcXedLQIcFgbERiooHEqM%3D')\n",
        "\n",
        "source = 'wasbs://dataset@dataset1000genomes.blob.core.windows.net/release/20130502/ALL.chr22*.vcf.gz'\n",
        "\n",
        "df = spark.read.\\\n",
        "  format('vcf').\\\n",
        "  option('includeSampleIds', True).\\\n",
        "  option('flattenInfoFields', True).\\\n",
        "  load(source)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We will use a subset of chromosome 22 to keep time and cost low. If you want to do format conversion for the whole chromosome 22 dataset you still can use this notebook but might need bigger cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "data=df.limit(10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Take a look at regular sites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "data.where('INFO_MULTI_ALLELIC = FALSE').show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "and multiallelic sites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "data.where('INFO_MULTI_ALLELIC = TRUE').show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "2. Transform data: add _hashId_ column (hash is built on all columns except genotypes and might be used as unique id for variants), explode on genotypes and flatten genotypes column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "hashCols = list(set(data.columns) - {'genotypes'})\r\n",
        "dataHashed = data.withColumn('hashId', xxhash64(*hashCols))\r\n",
        "\r\n",
        "dataExploded = dataHashed.withColumn('genotypes', explode('genotypes'))\r\n",
        "\r\n",
        "def flattenStructFields(df):\r\n",
        "  flat_cols = [c[0] for c in df.dtypes if c[1][:6] != 'struct']\r\n",
        "  nested_cols = [c[0] for c in df.dtypes if c[1][:6] =='struct']\r\n",
        "  flat_df = df.select(flat_cols + \r\n",
        "                     [col(nc+'.'+c).alias(nc+'_'+c)\r\n",
        "                     for nc in nested_cols\r\n",
        "                     for c in df.select(nc+'.*').columns])\r\n",
        "  return flat_df\r\n",
        "\r\n",
        "dataExplodedFlatten = flattenStructFields(dataExploded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Take a look at transformed dataset: regular sites\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dataExplodedFlatten.where('INFO_MULTI_ALLELIC = FALSE').show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "and multiallelic sites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dataExplodedFlatten.where('INFO_MULTI_ALLELIC = TRUE').show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "3. To save the data you need to provide names for output storage account, container and relative path, and set _outputPath_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "outputStorageAccount =  'Your account name' # replace with your account name\n",
        "outputContainer = 'Your container name' # replace with your container name\n",
        "outputDir = 'Your path' # replace with your relative path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Option 1:** If you use Azure Synapse Analytics and _outputStorageAccount_ is a primary (default) storage account in your Synapse workspace, run cell below\r\n",
        "\r\n",
        "Primary storage account is ADLS Gen2, so we use \r\n",
        "```\r\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "outputPath = 'abfss://%s@%s.dfs.core.windows.net/%s' % (outputContainer, outputStorageAccount, outputDir) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Option 2:** If you want to save data to Azure Blob Storage, provide SAS token for output container and run cell below. It works for Azure Databricks and Azure Synapse Analytics. \r\n",
        "\r\n",
        "For Azure Blob Storage we use  \r\n",
        "```\r\n",
        "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "outputSAS = 'Your SAS token' # replace with your SAS token\r\n",
        "spark.conf.set(\r\n",
        "  'fs.azure.sas.' + outputContainer + '.' + outputStorageAccount + '.blob.core.windows.net', outputSAS)\r\n",
        "\r\n",
        "outputPath = 'wasbs://%s@%s.blob.core.windows.net/%s' % (outputContainer, outputStorageAccount, outputDir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        " To learn more about Azure Blob Storage and Azure Data Lake Storage (ADLS) Gen2 and different ways to access them check documentation: \r\n",
        "\r\n",
        " - Azure Databricks - https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/ and https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-storage\r\n",
        " - Azure Synapse Analytics - https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "4. Write data in .parquet format to your storage account: original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7cb2db54-fcab-43c1-94f7-de6d64f20742"
        }
      },
      "source": [
        "sink = outputPath + '/original/chr22'\n",
        "dataHashed.write. \\\n",
        "  mode('overwrite'). \\\n",
        "  format('parquet'). \\\n",
        "  save(sink)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "and transformed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "091f7803-4fd9-4826-a8de-a9e6e423812a"
        }
      },
      "source": [
        "sink = outputPath + '/flattened/chr22' \n",
        "\n",
        "dataExplodedFlatten.write. \\\n",
        "  mode('overwrite'). \\\n",
        "  format('parquet'). \\\n",
        "  save(sink)"
      ]
    }
  ]
}