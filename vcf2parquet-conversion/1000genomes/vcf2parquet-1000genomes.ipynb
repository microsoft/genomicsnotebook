{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import explode, col, lit, xxhash64\n","from math import ceil\n","\n","# Import glow.py and register Glow package\n","import glow\n","glow.register(spark)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42f3599e-f63c-479f-92d7-846e09cd39ac"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Provide your storage account, container and SAS token\n","outputStorageAccount = \n","outputContainer = \n","outputSAS = \n","outputDir ="],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b1aced3-8185-4272-950c-ac28826997f3"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Configure session credentials\n","# Set up a SAS for a container with public data - no changes needed here (public SAS)\n","spark.conf.set(\n","  \"fs.azure.sas.dataset.dataset1000genomes.blob.core.windows.net\",\n","  \"sv=2019-10-10&si=prod&sr=c&sig=9nzcxaQn0NprMPlSh4RhFQHcXedLQIcFgbERiooHEqM%3D\")\n","\n","# Set up a SAS for a container to store .parquet files\n","spark.conf.set(\n","  \"fs.azure.sas.\"+outputContainer+\".\"+outputStorageAccount+\".blob.core.windows.net\", outputSAS)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32cd967d-57af-4c71-8249-6bb2a44c65f2"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# DropDuplicates() partitions into 200 pieces (default value)\n","# To change default number of partitions change config -  sqlContext.setConf(\"spark.sql.shuffle.partitions\", <YourNumberOfPartitions>)\n","partitionMax = 1500\n","sqlContext.setConf(\"spark.sql.shuffle.partitions\", partitionMax)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67bf49c0-865a-4b3b-8657-16f228047d47"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Flatten struct columns\n","def flattenStructFields(df):\n","  flat_cols = [c[0] for c in df.dtypes if c[1][:6] != 'struct']\n","  nested_cols = [c[0] for c in df.dtypes if c[1][:6] =='struct']\n","  flat_df = df.select(flat_cols + \n","                     [col(nc+'.'+c).alias(nc+'_'+c)\n","                     for nc in nested_cols\n","                     for c in df.select(nc+'.*').columns])\n","  return flat_df\n","\n","# Add empty columns to match schema\n","def completeSchema(df, diffSet):\n","  full_df = df\n","  for column in diffSet:\n","    full_df = full_df.withColumn(column.name, lit(None).cast(column.dataType.simpleString()))\n","  return full_df\n","\n","# Transform dataframe with original vcf schema\n","def transformVcf(df, toFlatten, toHash, fullSchemaFields):\n","  # Drop duplicates\n","  dataDedup = df.dropDuplicates()\n","     \n","  # Add hashId column to identify variants\n","  if toHash:\n","    hashCols = list(set(data.columns) - {'genotypes'})\n","    dataHashed = dataDedup.withColumn('hashId', xxhash64(*hashCols))\n","  else:\n","    dataHashed = dataDedup\n","  \n","  # Flatten data - explode on genotypes, create separate column for each genotypes field, add empty columns to match schema to full dataset\n","  if not toFlatten:\n","    dataFinal = dataHashed\n","  else:\n","  # Explode and flatten data\n","    dataExploded = dataHashed.withColumn('genotypes', explode('genotypes'))\n","    dataExplodedFlatten = flattenStructFields(dataExploded)\n","  # Find schema for contig dataset and add columns to match full schema\n","    contigSet = set(dataExplodedFlatten.schema.fields)\n","    diffSet =(fullSchemaFields - contigSet)\n","    dataFinal = completeSchema(dataExplodedFlatten, diffSet)\n","   \n","  return dataFinal"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"902378ed-6612-4baf-bc7b-0f98e3beacd0"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Create widgets for toFlatten and contigs\n","flatOptions = [False, True]\n","dbutils.widgets.dropdown(\"flatten\", \"False\", [str(x) for x in flatOptions])\n","\n","contigOptions =  list(map(str, range(1, 23)))\n","contigLiterals = ['X','Y','MT', 'All']\n","contigOptions.extend(contigLiterals)\n","dbutils.widgets.multiselect(\"contigsToProcess\", \"22\", contigOptions)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66eb007d-ee1e-40e9-be3e-f438bd23c470"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Define parameters\n","toFlatten = eval(getArgument(\"flatten\"))\n","toHash = True\n","repartitionCoef = 45 / 1000000 # gives ~20MB .parquet files\n","\n","# Define contig list\n","contigs = getArgument(\"contigsToProcess\").split(\",\")\n","if \"All\" in contigs:\n","  contigs = contigOptions\n","  contigs.remove('All')\n","\n","# Find schema for full dataset\n","sourceAll = \"wasbs://dataset@dataset1000genomes.blob.core.windows.net/release/20130502/ALL.chr*.vcf.gz\"\n","dataAll = spark.read\\\n","  .format(\"vcf\")\\\n","  .option(\"includeSampleIds\", True)\\\n","  .option(\"flattenInfoFields\", True)\\\n","  .load(sourceAll)\n","\n","dataAllExploded = dataAll.withColumn('genotypes', explode('genotypes'))\n","dataAllExplodedFlatten = flattenStructFields(dataAllExploded)\n","fullSet = set(dataAllExplodedFlatten.schema.fields)\n","                 \n","for contig in contigs:\n","  source = \"wasbs://dataset@dataset1000genomes.blob.core.windows.net/release/20130502/ALL.chr\"+contig+\".*.vcf.gz\"\n","\n","# Load data\n","  data = spark.read\\\n","    .format(\"vcf\")\\\n","    .option(\"includeSampleIds\", True)\\\n","    .option(\"flattenInfoFields\", True)\\\n","    .load(source)\n","  \n","  # Define number of partitions, will be used for coalesce later\n","  rowCount = data.count()\n","  partCount = ceil (repartitionCoef * rowCount)  \n","  if partCount > partitionMax:\n","    partCount = partitionMax\n","\n","  dataFinal = transformVcf(data, toFlatten, toHash, fullSet)\n","  if not toFlatten:\n","    sink = \"wasbs://\"+outputContainer + \"@\" + outputStorageAccount + \".blob.core.windows.net\"+ outputDir + \"/original/chr\"+contig\n","  else:\n","    sink = \"wasbs://\"+outputContainer + \"@\" + outputStorageAccount + \".blob.core.windows.net\"+ outputDir + \"/flattened/chr\"+contig\n","                 \n","  dataFinal.coalesce(partCount). \\\n","    write. \\\n","    mode(\"overwrite\"). \\\n","    format(\"parquet\"). \\\n","    save(sink)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70c077c0-10be-455b-839d-f7368bc4db29"}},"outputs":[],"execution_count":null}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"vcf2parquet-1000genomes","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"flatten":{"nuid":"8bf1ecc7-2a55-447a-80b7-aa1250b6c022","currentValue":"False","widgetInfo":{"widgetType":"dropdown","name":"flatten","defaultValue":"False","label":null,"options":{"widgetType":"dropdown","choices":["False","True"]}}},"contigsToProcess":{"nuid":"b716a858-a855-4c5a-8d3c-af0b05773029","currentValue":"22","widgetInfo":{"widgetType":"multiselect","name":"contigsToProcess","defaultValue":"22","label":null,"options":{"widgetType":"dropdown","choices":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","X","Y","MT","All"]}}}},"notebookOrigID":4372562996517358}},"nbformat":4,"nbformat_minor":0}