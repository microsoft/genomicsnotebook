{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "42f3599e-f63c-479f-92d7-846e09cd39ac"
        }
      },
      "source": [
        "from pyspark.sql.functions import col, xxhash64\n",
        "from notebookutils import mssparkutils\n",
        "import re\n",
        "\n",
        "# Import glow.py and register Glow package\n",
        "import glow\n",
        "spark = glow.register(spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "454af720-8d0f-4bed-86a3-5189471ef9b4"
        }
      },
      "source": [
        "# Provide names for output storage account, container and relative path\n",
        "\n",
        "outputStorageAccount = 'Your account name' # replace with your account name\n",
        "outputContainer = 'Your container name' # replace with your container name\n",
        "outputDir = 'Your path' # replace with your relative path\n",
        "\n",
        "\n",
        "# Here we assume that Azure Synapse Analytics is used and outputStorageAccount is a primary storage account in the workspace - no auth needed in this case \n",
        "# For other Synapse scenarios check https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python\n",
        "# For Azure Databricks check https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/ and https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-storage\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8b89be1f-bfa3-4abb-99dd-995a13535761"
        }
      },
      "source": [
        "# Define source public data\n",
        "\n",
        "inputStorageAccount = 'azureopendatastorage'\n",
        "inputContainer = 'gnomad'\n",
        "inputDir = 'release/2.1.1/vcf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f332ea7c-2a59-4c9d-b2a8-5969e443714e"
        }
      },
      "source": [
        "# Read, transform and write data\n",
        "\n",
        "def TransformData(source, sink, colsToDrop, colsToKeepAsArray):\n",
        "# Read data\n",
        "  data = spark.read. \\\n",
        "    format('vcf'). \\\n",
        "    load(source)\n",
        "# Drop columns\n",
        "  dataReduced = data\n",
        "  for column in colsToDrop:\n",
        "    dataReduced = dataReduced.drop(column)\n",
        "# Add hashId column\n",
        "  hashCols = dataReduced.columns\n",
        "  dataHashed = dataReduced.withColumn('hashId', xxhash64(*hashCols))\n",
        "# Replace arrays by first element  \n",
        "  colsToReplaceByFirstElement = []\n",
        "  for x, t in dataHashed.dtypes:\n",
        "    if t.startswith('array'):\n",
        "      colsToReplaceByFirstElement.append(x)\n",
        "  colsToReplaceByFirstElement = list(set(colsToReplaceByFirstElement) - set(colsToKeepAsArray))\n",
        "  dataTransformed = dataHashed\n",
        "  for column in colsToReplaceByFirstElement:\n",
        "    dataTransformed = dataTransformed.withColumn(column, col(column)[0])\n",
        "# Write data   \n",
        "  dataTransformed.write. \\\n",
        "    mode('overwrite'). \\\n",
        "    format('parquet'). \\\n",
        "    save(sink)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ebfd6d88-5604-468c-8c8f-cb67ba620e1e"
        }
      },
      "source": [
        "# Input files end with suffix 'chromosome.vcf.bgz', where chromosome might be a number from 1 to 22 or X or Y\n",
        "sourceSuffix = '[XY|0-9][.]vcf[.]bgz$'\n",
        "\n",
        "# Columns to drop and to keep as array\n",
        "colsToDrop = ['genotypes']\n",
        "colsToKeepAsArray = ['INFO_vep']\n",
        "\n",
        "# Datasets to process\n",
        "datasets = ['genomes']\n",
        "\n",
        "for dataset in datasets:\n",
        "  sourcePath = 'wasbs://%s@%s.blob.core.windows.net/%s/%s' % (inputContainer, inputStorageAccount, inputDir, dataset)   \n",
        "  files = mssparkutils.fs.ls(sourcePath)\n",
        "  for file in files:\n",
        "    if re.search(sourceSuffix, file.name):\n",
        "      source = file.path\n",
        "      sink = 'abfss://%s@%s.dfs.core.windows.net/%s/%s/%s' % (outputContainer, outputStorageAccount, outputDir, dataset, file.name.rstrip('.vcf.bgz')) \n",
        "      TransformData(source, sink, colsToDrop, colsToKeepAsArray)"
      ]
    }
  ]
}